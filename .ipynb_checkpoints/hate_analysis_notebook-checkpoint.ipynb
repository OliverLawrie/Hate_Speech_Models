{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d3e40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9d2e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('/mnt/data/hateXplain.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "df.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe093dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Data Cleaning\n",
    "# Check for missing values and drop them if necessary\n",
    "df.isnull().sum()  # Show the number of NaN values in each column\n",
    "df.dropna(inplace=True)  # Drop rows with missing values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494391d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Basic Text Preprocessing\n",
    "# Lowercasing, removing special characters, tokenization\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    text = ''.join([c for c in text if c.isalnum() or c.isspace()])  # Remove special characters\n",
    "    tokens = word_tokenize(text)  # Tokenize the text\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]  # Remove stopwords\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to the 'text' column\n",
    "df['comment'] = df['comment'].apply(preprocess_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f537213",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Sentiment Analysis using VADER\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to compute sentiment scores\n",
    "def get_sentiment(text):\n",
    "    sentiment = sia.polarity_scores(text)\n",
    "    return sentiment\n",
    "\n",
    "# Apply sentiment analysis to each comment and store the results\n",
    "df['sentiment'] = df['comment'].apply(get_sentiment)\n",
    "\n",
    "# Extract sentiment components into separate columns\n",
    "df['sentiment_neg'] = df['sentiment'].apply(lambda x: x['neg'])\n",
    "df['sentiment_neu'] = df['sentiment'].apply(lambda x: x['neu'])\n",
    "df['sentiment_pos'] = df['sentiment'].apply(lambda x: x['pos'])\n",
    "df['sentiment_compound'] = df['sentiment'].apply(lambda x: x['compound'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b0b6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Visualization\n",
    "# 4.1 Frequency of each type of hate (from the 'label' column)\n",
    "sns.countplot(x='label', data=df)\n",
    "plt.title('Frequency of Hate Types')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43c5d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4.2 Distribution of text length (number of words) for different hate categories\n",
    "df['comment_length'] = df['comment'].apply(lambda x: len(x.split()))\n",
    "sns.boxplot(x='label', y='comment_length', data=df)\n",
    "plt.title('Text Length Distribution by Hate Type')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbf6d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4.3 Most common words for each hate type\n",
    "hate_types = df['label'].unique()\n",
    "\n",
    "for hate_type in hate_types:\n",
    "    hate_comments = df[df['label'] == hate_type]['comment']\n",
    "    vectorizer = CountVectorizer(stop_words='english', max_features=10)\n",
    "    word_count = vectorizer.fit_transform(hate_comments)\n",
    "    word_freq = np.array(word_count.sum(axis=0)).flatten()\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "\n",
    "    word_freq_df = pd.DataFrame({'word': words, 'count': word_freq})\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.barplot(x='count', y='word', data=word_freq_df)\n",
    "    plt.title(f'Most Common Words in {hate_type} Comments')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848fdca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4.4 Sentiment analysis for each hate category\n",
    "sns.boxplot(x='label', y='sentiment_compound', data=df)\n",
    "plt.title('Sentiment Compound Scores by Hate Type')\n",
    "plt.show()\n",
    "    "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}